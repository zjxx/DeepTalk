<template>
  <div class="websocket-demo">
    <h1>ËØ≠Èü≥ËÅäÂ§©ÂÆ§</h1>
    
    <div class="connection-status">
      <div class="status-indicator" :class="{ connected: isConnected }"></div>
      <span>{{ connectionStatus }}</span>
      <button @click="toggleConnection" class="connect-btn">
        {{ isConnected ? 'Êñ≠ÂºÄËøûÊé•' : 'ËøûÊé•' }}
      </button>
    </div>

    <div class="voice-chat-container">
      <!-- Áî®Êà∑ÂàóË°® -->
      <div class="user-list">
        <h3>Âú®Á∫øÁî®Êà∑</h3>
        <div v-for="user in connectedUsers" :key="user.id" class="user-item">
          <span class="user-name">{{ user.name }}</span>
          <div class="voice-indicator" :class="{ active: user.isSpeaking }"></div>
        </div>
      </div>

      <!-- ËØ≠Èü≥ÊéßÂà∂Âå∫Âüü -->
      <div class="voice-controls">
        <button 
          @click="toggleMicrophone" 
          class="mic-btn"
          :class="{ 'recording': isRecording }"
          :disabled="!isConnected"
        >
          <span class="mic-icon">{{ isRecording ? 'üé§' : 'üîá' }}</span>
          {{ isRecording ? 'ÂÅúÊ≠¢ÂΩïÈü≥' : 'ÂºÄÂßãÂΩïÈü≥' }}
        </button>

        <!-- Èü≥ÈáèÊåáÁ§∫Âô® -->
        <div v-if="isRecording" class="audio-level-indicator">
          <div class="audio-level-bar" :style="{ width: `${audioLevel}%` }"></div>
        </div>
      </div>

      <!-- Ê∂àÊÅØÂéÜÂè≤ -->
      <div class="message-container">
        <div class="message-list" ref="messageList">
          <div v-for="(msg, index) in messages" :key="index" class="message">
            <span class="message-time">{{ msg.time }}</span>
            <span class="message-content">{{ msg.content }}</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref, onUnmounted, computed, nextTick } from 'vue'

interface User {
  id: string;
  name: string;
  isSpeaking: boolean;
}

const ws = ref<WebSocket | null>(null)
const isConnected = ref(false)
const messages = ref<Array<{ content: string; time: string }>>([])
const messageList = ref<HTMLElement | null>(null)
const connectedUsers = ref<User[]>([])
const isRecording = ref(false)
const audioLevel = ref(0)
const mediaRecorder = ref<MediaRecorder | null>(null)
const audioContext = ref<AudioContext | null>(null)
const audioAnalyser = ref<AnalyserNode | null>(null)
const audioDestination = ref<MediaStreamAudioDestinationNode | null>(null)
const audioChunks = ref<Blob[]>([])
const audioQueue = ref<ArrayBuffer[]>([])
const isPlaying = ref(false)
const userId = ref<string>(`user_${Math.random().toString(36).substr(2, 9)}`)
const sessionId = ref<string>('')

const connectionStatus = computed(() => {
  return isConnected.value ? 'Â∑≤ËøûÊé•' : 'Êú™ËøûÊé•'
})

const toggleConnection = () => {
  if (isConnected.value) {
    ws.value?.close()
  } else {
    connectWebSocket()
  }
}

const connectWebSocket = async () => {
  try {
    // È¶ñÂÖàËé∑ÂèñsessionId
    const response = await fetch('http://115.175.45.173:8080/api/speech/connect', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        userId: userId.value
      })
    })

    if (!response.ok) {
      throw new Error('Ëé∑ÂèñsessionIdÂ§±Ë¥•')
    }

    // Áõ¥Êé•‰ΩøÁî®ËøîÂõûÁöÑ token ‰Ωú‰∏∫ sessionId
    sessionId.value = await response.text()

    // ‰ΩøÁî®ÂÆåÊï¥URLÂª∫Á´ãWebSocketËøûÊé•
    const wsUrl = `ws://115.175.45.173:8080/api/speech/ws`
    ws.value = new WebSocket(wsUrl)
    
    ws.value.onopen = () => {
      // ËøûÊé•ÊàêÂäüÂêéÂèëÈÄÅÊ≥®ÂÜåÊ∂àÊÅØ
      ws.value?.send(JSON.stringify({
        type: 'register',
        userId: userId.value,
        sessionId: sessionId.value
      }))
      isConnected.value = true
      addMessage('Á≥ªÁªü', 'ËøûÊé•ÊàêÂäü')
    }
    
    ws.value.onmessage = async (event) => {
      // Ê£ÄÊü•Ê∂àÊÅØÁ±ªÂûã
      if (event.data instanceof Blob) {
        // Â§ÑÁêÜ‰∫åËøõÂà∂Èü≥È¢ëÊï∞ÊçÆ
        const audioData = event.data
        console.log('Êî∂Âà∞‰∫åËøõÂà∂Èü≥È¢ëÊï∞ÊçÆ:', {
          size: audioData.size,
          type: audioData.type
        })
        
        // Êí≠ÊîæÊé•Êî∂Âà∞ÁöÑÈü≥È¢ëÊï∞ÊçÆ
        await playAudio(audioData)
      } else {
        // Â§ÑÁêÜÊñáÊú¨Ê∂àÊÅØÔºàÁ≥ªÁªüÊ∂àÊÅØÁ≠âÔºâ
        const data = JSON.parse(event.data)
        
        switch (data.type) {
          case 'system': {
            addMessage('Á≥ªÁªü', data.message)
            break
          }
          case 'user_join': {
            connectedUsers.value.push({
              id: data.userId,
              name: data.userName,
              isSpeaking: false
            })
            addMessage('Á≥ªÁªü', `${data.userName} Âä†ÂÖ•‰∫ÜËÅäÂ§©ÂÆ§`)
            break
          }
          case 'user_leave': {
            connectedUsers.value = connectedUsers.value.filter(u => u.id !== data.userId)
            addMessage('Á≥ªÁªü', `${data.userName} Á¶ªÂºÄ‰∫ÜËÅäÂ§©ÂÆ§`)
            break
          }
        }
      }
    }
    
    ws.value.onclose = () => {
      isConnected.value = false
      addMessage('Á≥ªÁªü', 'ËøûÊé•Â∑≤Êñ≠ÂºÄ')
      stopRecording()
    }
    
    ws.value.onerror = (error) => {
      console.error('WebSocketÈîôËØØ:', error)
      addMessage('Á≥ªÁªü', 'ËøûÊé•ÂèëÁîüÈîôËØØ')
    }
  } catch (error) {
    console.error('ÂàõÂª∫WebSocketËøûÊé•Â§±Ë¥•:', error)
    addMessage('Á≥ªÁªü', 'ÂàõÂª∫ËøûÊé•Â§±Ë¥•')
  }
}

const toggleMicrophone = async () => {
  if (isRecording.value) {
    stopRecording()
  } else {
    startRecording()
  }
}

// eslint-disable-next-line @typescript-eslint/no-unused-vars
const checkAudioSupport = () => {
  const mimeTypes = [
    'audio/webm;codecs=opus',
    'audio/webm',
    'audio/ogg;codecs=opus',
    'audio/mp4'
  ]
  
  const supportedType = mimeTypes.find(type => MediaRecorder.isTypeSupported(type))
  if (!supportedType) {
    console.warn('ÊµèËßàÂô®‰∏çÊîØÊåÅ‰ªª‰ΩïÈü≥È¢ëÊ†ºÂºèÔºåÂ∞Ü‰ΩøÁî®ÈªòËÆ§Ê†ºÂºè')
    return null
  }
  
  console.log('‰ΩøÁî®Èü≥È¢ëÊ†ºÂºè:', supportedType)
  return supportedType
}

// ÂàùÂßãÂåñÈü≥È¢ë‰∏ä‰∏ãÊñá
const initAudioContext = () => {
  if (!audioContext.value) {
    audioContext.value = new AudioContext()
    audioDestination.value = audioContext.value.createMediaStreamDestination()
  }
  return audioContext.value
}

// Êí≠ÊîæÈü≥È¢ëÈòüÂàó
const playNextAudio = async () => {
  if (isPlaying.value || audioQueue.value.length === 0) return
  
  try {
    isPlaying.value = true
    const audioData = audioQueue.value.shift()
    if (!audioData) {
      console.log('Ê≤°ÊúâÂèØÊí≠ÊîæÁöÑÈü≥È¢ëÊï∞ÊçÆ')
      return
    }
    
    console.log('ÂáÜÂ§áÊí≠ÊîæÈü≥È¢ëÊï∞ÊçÆ:', {
      size: audioData.byteLength,
      firstBytes: Array.from(new Uint8Array(audioData.slice(0, 10)))
    })
    
    const context = initAudioContext()
    
    // ‰ΩøÁî® AudioContext Ëß£Á†ÅÈü≥È¢ëÊï∞ÊçÆ
    try {
      const audioBuffer = await context.decodeAudioData(audioData.slice(0))
      console.log('Èü≥È¢ëËß£Á†ÅÊàêÂäü:', {
        duration: audioBuffer.duration,
        numberOfChannels: audioBuffer.numberOfChannels,
        sampleRate: audioBuffer.sampleRate
      })
      
      const source = context.createBufferSource()
      source.buffer = audioBuffer
      
      // ÂàõÂª∫Â¢ûÁõäËäÇÁÇπÊù•ÊéßÂà∂Èü≥Èáè
      const gainNode = context.createGain()
      gainNode.gain.value = 1.0 // ÂèØ‰ª•Ë∞ÉÊï¥Èü≥Èáè
      
      // ËøûÊé•ËäÇÁÇπ
      source.connect(gainNode)
      gainNode.connect(context.destination)
      
      source.onended = () => {
        console.log('Èü≥È¢ëÊí≠ÊîæÂÆåÊàê')
        isPlaying.value = false
        playNextAudio() // Êí≠ÊîæÈòüÂàó‰∏≠ÁöÑ‰∏ã‰∏Ä‰∏™Èü≥È¢ë
      }
      
      source.start(0)
      console.log('ÂºÄÂßãÊí≠ÊîæÈü≥È¢ë')
    } catch (decodeError) {
      console.error('Èü≥È¢ëËß£Á†ÅÂ§±Ë¥•:', {
        error: decodeError,
        dataSize: audioData.byteLength,
        firstBytes: Array.from(new Uint8Array(audioData.slice(0, 20)))
      })
      throw decodeError
    }
  } catch (error) {
    console.error('Èü≥È¢ëÊí≠ÊîæÂ§±Ë¥•:', error)
    isPlaying.value = false
    // Â¶ÇÊûúËß£Á†ÅÂ§±Ë¥•ÔºåÂ∞ùËØïÊí≠Êîæ‰∏ã‰∏Ä‰∏™
    setTimeout(() => playNextAudio(), 100)
  }
}

const playAudio = async (audioBlob: Blob) => {
  try {
    console.log('ÂºÄÂßãÂ§ÑÁêÜÊé•Êî∂Âà∞ÁöÑÈü≥È¢ëÊï∞ÊçÆ')
    
    // ‰ΩøÁî® FileReader ËØªÂèñ Blob Êï∞ÊçÆ
    const reader = new FileReader()
    reader.onload = async (e) => {
      try {
        const arrayBuffer = e.target?.result as ArrayBuffer
        if (!arrayBuffer) {
          console.error('FileReader Ê≤°ÊúâËøîÂõûÊï∞ÊçÆ')
          return
        }
        
        console.log('FileReader ËØªÂèñÂÆåÊàê:', {
          size: arrayBuffer.byteLength,
          firstBytes: Array.from(new Uint8Array(arrayBuffer.slice(0, 10)))
        })
        
        // Â∞ÜÊï∞ÊçÆÊ∑ªÂä†Âà∞Êí≠ÊîæÈòüÂàó
        audioQueue.value.push(arrayBuffer)
        console.log('ÂΩìÂâçÈü≥È¢ëÈòüÂàóÈïøÂ∫¶:', audioQueue.value.length)
        
        // Â¶ÇÊûúÂΩìÂâçÊ≤°ÊúâÂú®Êí≠ÊîæÔºåÂºÄÂßãÊí≠Êîæ
        if (!isPlaying.value) {
          await playNextAudio()
        }
      } catch (error) {
        console.error('Èü≥È¢ëÊï∞ÊçÆÂ§ÑÁêÜÂ§±Ë¥•:', error)
      }
    }
    
    reader.onerror = (error) => {
      console.error('Êñá‰ª∂ËØªÂèñÂ§±Ë¥•:', error)
    }
    
    reader.readAsArrayBuffer(audioBlob)
  } catch (error) {
    console.error('Èü≥È¢ëÊï∞ÊçÆÂ§ÑÁêÜÂ§±Ë¥•:', error)
  }
}

const startRecording = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ 
      audio: {
        channelCount: 1,
        sampleRate: 44100,
        sampleSize: 16,
        echoCancellation: true,
        noiseSuppression: true
      } 
    })
    
    // ÂàùÂßãÂåñÈü≥È¢ë‰∏ä‰∏ãÊñá
    const context = initAudioContext()
    const source = context.createMediaStreamSource(stream)
    audioAnalyser.value = context.createAnalyser()
    audioAnalyser.value.fftSize = 256
    source.connect(audioAnalyser.value)
    
    // ÂºÄÂßãÂΩïÈü≥Ôºå‰ΩøÁî®Âõ∫ÂÆöÁöÑÈü≥È¢ëÊ†ºÂºè
    const options = {
      mimeType: 'audio/webm;codecs=opus',
      audioBitsPerSecond: 128000
    }
    
    try {
      mediaRecorder.value = new MediaRecorder(stream, options)
    } catch (e) {
      console.warn('Êó†Ê≥ï‰ΩøÁî®È¶ñÈÄâÊ†ºÂºèÔºåÂ∞ùËØï‰ΩøÁî®ÈªòËÆ§Ê†ºÂºè:', e)
      mediaRecorder.value = new MediaRecorder(stream)
    }
    
    audioChunks.value = []
    
    mediaRecorder.value.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.value.push(event.data)
      }
    }

    mediaRecorder.value.onstop = () => {
      if (audioChunks.value.length > 0) {
        const audioBlob = new Blob(audioChunks.value, { type: 'audio/webm;codecs=opus' })
        sendAudioData(audioBlob)
        audioChunks.value = []
      }
    }

    mediaRecorder.value.start()
    isRecording.value = true
    
    // ÂºÄÂßãÈü≥ÈáèÁõëÊµã
    monitorAudioLevel()
  } catch (error) {
    console.error('ÂêØÂä®ÂΩïÈü≥Â§±Ë¥•:', error)
    addMessage('Á≥ªÁªü', 'Êó†Ê≥ïËÆøÈóÆÈ∫¶ÂÖãÈ£é')
  }
}

const stopRecording = () => {
  if (mediaRecorder.value && isRecording.value) {
    mediaRecorder.value.stop()
    mediaRecorder.value.stream.getTracks().forEach(track => track.stop())
    isRecording.value = false
    audioLevel.value = 0
    
    // Ê∏ÖÁêÜÈü≥È¢ë‰∏ä‰∏ãÊñá
    if (audioContext.value) {
      audioContext.value.close()
      audioContext.value = null
      audioDestination.value = null
    }
  }
}

const monitorAudioLevel = () => {
  if (!audioAnalyser.value || !isRecording.value) return
  
  const dataArray = new Uint8Array(audioAnalyser.value.frequencyBinCount)
  const updateLevel = () => {
    if (!audioAnalyser.value || !isRecording.value) return
    
    audioAnalyser.value.getByteFrequencyData(dataArray)
    const average = dataArray.reduce((a, b) => a + b) / dataArray.length
    audioLevel.value = Math.min(100, (average / 128) * 100)
    
    requestAnimationFrame(updateLevel)
  }
  
  updateLevel()
}

const sendAudioData = async (audioData: Blob) => {
  if (ws.value?.readyState === WebSocket.OPEN) {
    try {
      const arrayBuffer = await audioData.arrayBuffer()
      
      // Ê£ÄÊü•Êï∞ÊçÆÂ§ßÂ∞è
      if (arrayBuffer.byteLength < 100) {
        console.log('Èü≥È¢ëÊï∞ÊçÆÂ§™Â∞èÔºåË∑≥ËøáÂèëÈÄÅ')
        return
      }
      
      console.log('ÂèëÈÄÅÈü≥È¢ëÊï∞ÊçÆ:', {
        size: arrayBuffer.byteLength,
        firstBytes: Array.from(new Uint8Array(arrayBuffer.slice(0, 10))),
        isWebM: new Uint8Array(arrayBuffer)[0] === 0x1A && 
                new Uint8Array(arrayBuffer)[1] === 0x45 && 
                new Uint8Array(arrayBuffer)[2] === 0xDF && 
                new Uint8Array(arrayBuffer)[3] === 0xA3,
        mimeType: audioData.type
      })
      
      // Áõ¥Êé•ÂèëÈÄÅ‰∫åËøõÂà∂Êï∞ÊçÆ
      ws.value.send(arrayBuffer)
    } catch (error) {
      console.error('ÂèëÈÄÅÈü≥È¢ëÊï∞ÊçÆÂ§±Ë¥•:', error)
    }
  }
}

const addMessage = (sender: string, content: string) => {
  const time = new Date().toLocaleTimeString()
  messages.value.push({ content: `${sender}: ${content}`, time })

  nextTick(() => {
    if (messageList.value) {
      messageList.value.scrollTop = messageList.value.scrollHeight
    }
  })
}

onUnmounted(() => {
  stopRecording()
  if (audioContext.value) {
    audioContext.value.close()
  }
  if (ws.value) {
    ws.value.close()
  }
})
</script>

<style scoped>
.websocket-demo {
  max-width: 800px;
  margin: 20px auto;
  padding: 20px;
}

.connection-status {
  display: flex;
  align-items: center;
  gap: 10px;
  margin-bottom: 20px;
}

.status-indicator {
  width: 12px;
  height: 12px;
  border-radius: 50%;
  background-color: #ff4444;
}

.status-indicator.connected {
  background-color: #00ff00;
}

.connect-btn {
  padding: 8px 16px;
  border-radius: 4px;
  border: none;
  background-color: #4CAF50;
  color: white;
  cursor: pointer;
}

.connect-btn:hover {
  background-color: #45a049;
}

.voice-chat-container {
  display: grid;
  grid-template-columns: 200px 1fr;
  gap: 20px;
  margin-top: 20px;
}

.user-list {
  background-color: #f5f5f5;
  padding: 15px;
  border-radius: 8px;
}

.user-item {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 8px;
  margin-bottom: 8px;
  background-color: white;
  border-radius: 4px;
}

.voice-indicator {
  width: 8px;
  height: 8px;
  border-radius: 50%;
  background-color: #ccc;
}

.voice-indicator.active {
  background-color: #4CAF50;
  animation: pulse 1s infinite;
}

.voice-controls {
  grid-column: 1 / -1;
  display: flex;
  align-items: center;
  gap: 20px;
  padding: 15px;
  background-color: #f5f5f5;
  border-radius: 8px;
}

.mic-btn {
  padding: 12px 24px;
  border: none;
  border-radius: 25px;
  background-color: #2196F3;
  color: white;
  cursor: pointer;
  display: flex;
  align-items: center;
  gap: 8px;
  transition: all 0.3s ease;
}

.mic-btn.recording {
  background-color: #f44336;
  animation: pulse 1.5s infinite;
}

.audio-level-indicator {
  flex: 1;
  height: 8px;
  background-color: #ddd;
  border-radius: 4px;
  overflow: hidden;
}

.audio-level-bar {
  height: 100%;
  background: linear-gradient(90deg, #4CAF50 0%, #FFC107 50%, #F44336 100%);
  transition: width 0.1s ease;
}

.message-container {
  grid-column: 1 / -1;
  border: 1px solid #ddd;
  border-radius: 8px;
  overflow: hidden;
}

.message-list {
  height: 300px;
  overflow-y: auto;
  padding: 15px;
  background-color: #f9f9f9;
}

.message {
  margin-bottom: 10px;
  padding: 8px;
  background-color: white;
  border-radius: 4px;
  box-shadow: 0 1px 2px rgba(0,0,0,0.1);
}

.message-time {
  font-size: 0.8em;
  color: #666;
  margin-right: 8px;
}

@keyframes pulse {
  0% {
    transform: scale(1);
    opacity: 1;
  }
  50% {
    transform: scale(1.1);
    opacity: 0.8;
  }
  100% {
    transform: scale(1);
    opacity: 1;
  }
}
</style>