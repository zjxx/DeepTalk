<template>
    <div class="websocket-demo">
      <h1>è¯­éŸ³èŠå¤©å®¤</h1>
      
      <div class="connection-status">
        <div class="status-indicator" :class="{ connected: isConnected }"></div>
        <span>{{ connectionStatus }}</span>
        <button @click="toggleConnection" class="connect-btn">
          {{ isConnected ? 'æ–­å¼€è¿æ¥' : 'è¿æ¥' }}
        </button>
      </div>
  
      <div class="voice-chat-container">
        <!-- ç”¨æˆ·åˆ—è¡¨ -->
        <div class="user-list">
          <h3>åœ¨çº¿ç”¨æˆ·</h3>
          <div v-for="user in connectedUsers" :key="user.id" class="user-item">
            <span class="user-name">{{ user.name }}</span>
            <div class="voice-indicator" :class="{ active: user.isSpeaking }"></div>
          </div>
        </div>
  
        <!-- è¯­éŸ³æ§åˆ¶åŒºåŸŸ -->
        <div class="voice-controls">
          <button 
            @click="toggleMicrophone" 
            class="mic-btn"
            :class="{ 'recording': isRecording }"
            :disabled="!isConnected"
          >
            <span class="mic-icon">{{ isRecording ? 'ğŸ¤' : 'ğŸ”‡' }}</span>
            {{ isRecording ? 'åœæ­¢å½•éŸ³' : 'å¼€å§‹å½•éŸ³' }}
          </button>
  
          <!-- éŸ³é‡æŒ‡ç¤ºå™¨ -->
          <div v-if="isRecording" class="audio-level-indicator">
            <div class="audio-level-bar" :style="{ width: `${audioLevel}%` }"></div>
          </div>
        </div>
  
        <!-- æ¶ˆæ¯å†å² -->
        <div class="message-container">
          <div class="message-list" ref="messageList">
            <div v-for="(msg, index) in messages" :key="index" class="message">
              <span class="message-time">{{ msg.time }}</span>
              <span class="message-content">{{ msg.content }}</span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </template>
  
  <script setup lang="ts">
  import { ref, onUnmounted, computed, nextTick } from 'vue'
  
  interface User {
    id: string;
    name: string;
    isSpeaking: boolean;
  }
  
  const ws = ref<WebSocket | null>(null)
  const isConnected = ref(false)
  const messages = ref<Array<{ content: string; time: string }>>([])
  const messageList = ref<HTMLElement | null>(null)
  const connectedUsers = ref<User[]>([])
  const isRecording = ref(false)
  const audioLevel = ref(0)
  const mediaRecorder = ref<MediaRecorder | null>(null)
  const audioContext = ref<AudioContext | null>(null)
  const audioAnalyser = ref<AnalyserNode | null>(null)
  const audioDestination = ref<MediaStreamAudioDestinationNode | null>(null)
  const audioChunks = ref<Blob[]>([])
  const audioQueue = ref<ArrayBuffer[]>([])
  const isPlaying = ref(false)
  const userId = ref<string>(`user_${Math.random().toString(36).substr(2, 9)}`)
  
  const connectionStatus = computed(() => {
    return isConnected.value ? 'å·²è¿æ¥' : 'æœªè¿æ¥'
  })
  
  const toggleConnection = () => {
    if (isConnected.value) {
      ws.value?.close()
    } else {
      connectWebSocket()
    }
  }
  
  const connectWebSocket = () => {
    const wsUrl = 'ws://localhost:8765'
  
    try {
      ws.value = new WebSocket(wsUrl)
      
      ws.value.onopen = () => {
        isConnected.value = true
        addMessage('ç³»ç»Ÿ', 'è¿æ¥æˆåŠŸ')
        // å‘é€ç”¨æˆ·åŠ å…¥æ¶ˆæ¯
        sendUserJoin()
      }
      
      ws.value.onmessage = async (event) => {
        const data = JSON.parse(event.data)
        
        switch (data.type) {
          case 'system': {
            addMessage('ç³»ç»Ÿ', data.message)
            break
          }
          case 'user_join': {
            connectedUsers.value.push({
              id: data.userId,
              name: data.userName,
              isSpeaking: false
            })
            addMessage('ç³»ç»Ÿ', `${data.userName} åŠ å…¥äº†èŠå¤©å®¤`)
            break
          }
          case 'user_leave': {
            connectedUsers.value = connectedUsers.value.filter(u => u.id !== data.userId)
            addMessage('ç³»ç»Ÿ', `${data.userName} ç¦»å¼€äº†èŠå¤©å®¤`)
            break
          }
          case 'voice': {
            console.log('æ”¶åˆ°è¯­éŸ³æ•°æ®:', {
              userId: data.userId,
              dataSize: data.audioData.length,
              timestamp: data.metadata?.timestamp,
              format: data.format
            })
            // å¦‚æœä¸æ˜¯è‡ªå·±å‘é€çš„éŸ³é¢‘æ‰æ’­æ”¾
            if (data.userId !== userId.value) {
              // æ’­æ”¾æ¥æ”¶åˆ°çš„è¯­éŸ³æ•°æ®
              await playAudio(data.audioData)
            } else {
              console.log('è·³è¿‡æ’­æ”¾è‡ªå·±å‘é€çš„éŸ³é¢‘')
            }
            // æ›´æ–°è¯´è¯çŠ¶æ€
            const user = connectedUsers.value.find(u => u.id === data.userId)
            if (user) {
              user.isSpeaking = true
              setTimeout(() => {
                user.isSpeaking = false
              }, 1000)
            }
            break
          }
        }
      }
      
      ws.value.onclose = () => {
        isConnected.value = false
        addMessage('ç³»ç»Ÿ', 'è¿æ¥å·²æ–­å¼€')
        stopRecording()
      }
      
      ws.value.onerror = (error) => {
        console.error('WebSocketé”™è¯¯:', error)
        addMessage('ç³»ç»Ÿ', 'è¿æ¥å‘ç”Ÿé”™è¯¯')
      }
    } catch (error) {
      console.error('åˆ›å»ºWebSocketè¿æ¥å¤±è´¥:', error)
      addMessage('ç³»ç»Ÿ', 'åˆ›å»ºè¿æ¥å¤±è´¥')
    }
  }
  
  const sendUserJoin = () => {
    if (ws.value?.readyState === WebSocket.OPEN) {
      ws.value.send(JSON.stringify({
        type: 'user_join',
        userId: userId.value,
        userName: `ç”¨æˆ·${userId.value.slice(-4)}`
      }))
    }
  }
  
  const toggleMicrophone = async () => {
    if (isRecording.value) {
      stopRecording()
    } else {
      startRecording()
    }
  }
  
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  const checkAudioSupport = () => {
    const mimeTypes = [
      'audio/webm;codecs=opus',
      'audio/webm',
      'audio/ogg;codecs=opus',
      'audio/mp4'
    ]
    
    const supportedType = mimeTypes.find(type => MediaRecorder.isTypeSupported(type))
    if (!supportedType) {
      console.warn('æµè§ˆå™¨ä¸æ”¯æŒä»»ä½•éŸ³é¢‘æ ¼å¼ï¼Œå°†ä½¿ç”¨é»˜è®¤æ ¼å¼')
      return null
    }
    
    console.log('ä½¿ç”¨éŸ³é¢‘æ ¼å¼:', supportedType)
    return supportedType
  }
  
  // åˆå§‹åŒ–éŸ³é¢‘ä¸Šä¸‹æ–‡
  const initAudioContext = () => {
    if (!audioContext.value) {
      audioContext.value = new AudioContext()
      audioDestination.value = audioContext.value.createMediaStreamDestination()
    }
    return audioContext.value
  }
  
  // æ’­æ”¾éŸ³é¢‘é˜Ÿåˆ—
  const playNextAudio = async () => {
    if (isPlaying.value || audioQueue.value.length === 0) return
    
    try {
      isPlaying.value = true
      const audioData = audioQueue.value.shift()
      if (!audioData) {
        console.log('æ²¡æœ‰å¯æ’­æ”¾çš„éŸ³é¢‘æ•°æ®')
        return
      }
      
      console.log('å‡†å¤‡æ’­æ”¾éŸ³é¢‘æ•°æ®:', {
        size: audioData.byteLength,
        firstBytes: Array.from(new Uint8Array(audioData.slice(0, 10)))
      })
      
      const context = initAudioContext()
      
      // ä½¿ç”¨ AudioContext è§£ç éŸ³é¢‘æ•°æ®
      try {
        const audioBuffer = await context.decodeAudioData(audioData.slice(0))
        console.log('éŸ³é¢‘è§£ç æˆåŠŸ:', {
          duration: audioBuffer.duration,
          numberOfChannels: audioBuffer.numberOfChannels,
          sampleRate: audioBuffer.sampleRate
        })
        
        const source = context.createBufferSource()
        source.buffer = audioBuffer
        
        // åˆ›å»ºå¢ç›ŠèŠ‚ç‚¹æ¥æ§åˆ¶éŸ³é‡
        const gainNode = context.createGain()
        gainNode.gain.value = 1.0 // å¯ä»¥è°ƒæ•´éŸ³é‡
        
        // è¿æ¥èŠ‚ç‚¹
        source.connect(gainNode)
        gainNode.connect(context.destination)
        
        source.onended = () => {
          console.log('éŸ³é¢‘æ’­æ”¾å®Œæˆ')
          isPlaying.value = false
          playNextAudio() // æ’­æ”¾é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªéŸ³é¢‘
        }
        
        source.start(0)
        console.log('å¼€å§‹æ’­æ”¾éŸ³é¢‘')
      } catch (decodeError) {
        console.error('éŸ³é¢‘è§£ç å¤±è´¥:', {
          error: decodeError,
          dataSize: audioData.byteLength,
          firstBytes: Array.from(new Uint8Array(audioData.slice(0, 20)))
        })
        throw decodeError
      }
    } catch (error) {
      console.error('éŸ³é¢‘æ’­æ”¾å¤±è´¥:', error)
      isPlaying.value = false
      // å¦‚æœè§£ç å¤±è´¥ï¼Œå°è¯•æ’­æ”¾ä¸‹ä¸€ä¸ª
      setTimeout(() => playNextAudio(), 100)
    }
  }
  
  const playAudio = async (base64Data: string) => {
    try {
      console.log('å¼€å§‹å¤„ç†æ¥æ”¶åˆ°çš„éŸ³é¢‘æ•°æ®')
      
      // è§£ç  base64 æ•°æ®
      const binaryString = atob(base64Data)
      const bytes = new Uint8Array(binaryString.length)
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i)
      }
      
      console.log('è§£ç åçš„éŸ³é¢‘æ•°æ®:', {
        size: bytes.length,
        firstBytes: Array.from(bytes.slice(0, 10)), // æ˜¾ç¤ºå‰10ä¸ªå­—èŠ‚
        isWebM: bytes[0] === 0x1A && bytes[1] === 0x45 && bytes[2] === 0xDF && bytes[3] === 0xA3 // WebMæ–‡ä»¶å¤´æ£€æŸ¥
      })
      
      // åˆ›å»º Blob å¯¹è±¡
      const blob = new Blob([bytes], { type: 'audio/webm;codecs=opus' })
      console.log('åˆ›å»ºçš„Blob:', {
        size: blob.size,
        type: blob.type
      })
      
      // ä½¿ç”¨ FileReader è¯»å– Blob æ•°æ®
      const reader = new FileReader()
      reader.onload = async (e) => {
        try {
          const arrayBuffer = e.target?.result as ArrayBuffer
          if (!arrayBuffer) {
            console.error('FileReader æ²¡æœ‰è¿”å›æ•°æ®')
            return
          }
          
          console.log('FileReader è¯»å–å®Œæˆ:', {
            size: arrayBuffer.byteLength,
            firstBytes: Array.from(new Uint8Array(arrayBuffer.slice(0, 10)))
          })
          
          // å°†æ•°æ®æ·»åŠ åˆ°æ’­æ”¾é˜Ÿåˆ—
          audioQueue.value.push(arrayBuffer)
          console.log('å½“å‰éŸ³é¢‘é˜Ÿåˆ—é•¿åº¦:', audioQueue.value.length)
          
          // å¦‚æœå½“å‰æ²¡æœ‰åœ¨æ’­æ”¾ï¼Œå¼€å§‹æ’­æ”¾
          if (!isPlaying.value) {
            await playNextAudio()
          }
        } catch (error) {
          console.error('éŸ³é¢‘æ•°æ®å¤„ç†å¤±è´¥:', error)
        }
      }
      
      reader.onerror = (error) => {
        console.error('æ–‡ä»¶è¯»å–å¤±è´¥:', error)
      }
      
      reader.readAsArrayBuffer(blob)
    } catch (error) {
      console.error('éŸ³é¢‘æ•°æ®å¤„ç†å¤±è´¥:', error)
    }
  }
  
  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          channelCount: 1,
          sampleRate: 44100,
          sampleSize: 16,
          echoCancellation: true,
          noiseSuppression: true
        } 
      })
      
      // åˆå§‹åŒ–éŸ³é¢‘ä¸Šä¸‹æ–‡
      const context = initAudioContext()
      const source = context.createMediaStreamSource(stream)
      audioAnalyser.value = context.createAnalyser()
      audioAnalyser.value.fftSize = 256
      source.connect(audioAnalyser.value)
      
      // å¼€å§‹å½•éŸ³ï¼Œä½¿ç”¨å›ºå®šçš„éŸ³é¢‘æ ¼å¼
      const options = {
        mimeType: 'audio/webm;codecs=opus',
        audioBitsPerSecond: 128000
      }
      
      try {
        mediaRecorder.value = new MediaRecorder(stream, options)
      } catch (e) {
        console.warn('æ— æ³•ä½¿ç”¨é¦–é€‰æ ¼å¼ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤æ ¼å¼:', e)
        mediaRecorder.value = new MediaRecorder(stream)
      }
      
      audioChunks.value = []
      
      mediaRecorder.value.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunks.value.push(event.data)
        }
      }

      mediaRecorder.value.onstop = () => {
        if (audioChunks.value.length > 0) {
          const audioBlob = new Blob(audioChunks.value, { type: 'audio/webm;codecs=opus' })
          sendAudioData(audioBlob)
          audioChunks.value = []
        }
      }

      mediaRecorder.value.start()
      isRecording.value = true
      
      // å¼€å§‹éŸ³é‡ç›‘æµ‹
      monitorAudioLevel()
    } catch (error) {
      console.error('å¯åŠ¨å½•éŸ³å¤±è´¥:', error)
      addMessage('ç³»ç»Ÿ', 'æ— æ³•è®¿é—®éº¦å…‹é£')
    }
  }
  
  const stopRecording = () => {
    if (mediaRecorder.value && isRecording.value) {
      mediaRecorder.value.stop()
      mediaRecorder.value.stream.getTracks().forEach(track => track.stop())
      isRecording.value = false
      audioLevel.value = 0
      
      // æ¸…ç†éŸ³é¢‘ä¸Šä¸‹æ–‡
      if (audioContext.value) {
        audioContext.value.close()
        audioContext.value = null
        audioDestination.value = null
      }
    }
  }
  
  const monitorAudioLevel = () => {
    if (!audioAnalyser.value || !isRecording.value) return
    
    const dataArray = new Uint8Array(audioAnalyser.value.frequencyBinCount)
    const updateLevel = () => {
      if (!audioAnalyser.value || !isRecording.value) return
      
      audioAnalyser.value.getByteFrequencyData(dataArray)
      const average = dataArray.reduce((a, b) => a + b) / dataArray.length
      audioLevel.value = Math.min(100, (average / 128) * 100)
      
      requestAnimationFrame(updateLevel)
    }
    
    updateLevel()
  }
  
  const sendAudioData = async (audioData: Blob) => {
    if (ws.value?.readyState === WebSocket.OPEN) {
      try {
        const arrayBuffer = await audioData.arrayBuffer()
        const uint8Array = new Uint8Array(arrayBuffer)
        
        // æ£€æŸ¥æ•°æ®å¤§å°
        if (arrayBuffer.byteLength < 100) {
          console.log('éŸ³é¢‘æ•°æ®å¤ªå°ï¼Œè·³è¿‡å‘é€')
          return
        }
        
        console.log('å‘é€éŸ³é¢‘æ•°æ®:', {
          size: arrayBuffer.byteLength,
          firstBytes: Array.from(uint8Array.slice(0, 10)),
          isWebM: uint8Array[0] === 0x1A && uint8Array[1] === 0x45 && uint8Array[2] === 0xDF && uint8Array[3] === 0xA3,
          mimeType: audioData.type
        })
        
        // å°†ArrayBufferè½¬æ¢ä¸ºbase64
        const base64Data = btoa(
          Array.from(uint8Array).map(byte => String.fromCharCode(byte)).join('')
        )
        
        ws.value.send(JSON.stringify({
          type: 'voice',
          userId: userId.value,
          audioData: base64Data,
          format: audioData.type,
          metadata: {
            size: arrayBuffer.byteLength,
            timestamp: Date.now(),
            mimeType: audioData.type
          }
        }))
      } catch (error) {
        console.error('å‘é€éŸ³é¢‘æ•°æ®å¤±è´¥:', error)
      }
    }
  }
  
  const addMessage = (sender: string, content: string) => {
    const time = new Date().toLocaleTimeString()
    messages.value.push({ content: `${sender}: ${content}`, time })
  
    nextTick(() => {
      if (messageList.value) {
        messageList.value.scrollTop = messageList.value.scrollHeight
      }
    })
  }
  
  onUnmounted(() => {
    stopRecording()
    if (audioContext.value) {
      audioContext.value.close()
    }
    if (ws.value) {
      ws.value.close()
    }
  })
  </script>
  
  <style scoped>
  .websocket-demo {
    max-width: 800px;
    margin: 20px auto;
    padding: 20px;
  }
  
  .connection-status {
    display: flex;
    align-items: center;
    gap: 10px;
    margin-bottom: 20px;
  }
  
  .status-indicator {
    width: 12px;
    height: 12px;
    border-radius: 50%;
    background-color: #ff4444;
  }
  
  .status-indicator.connected {
    background-color: #00ff00;
  }
  
  .connect-btn {
    padding: 8px 16px;
    border-radius: 4px;
    border: none;
    background-color: #4CAF50;
    color: white;
    cursor: pointer;
  }
  
  .connect-btn:hover {
    background-color: #45a049;
  }
  
  .voice-chat-container {
    display: grid;
    grid-template-columns: 200px 1fr;
    gap: 20px;
    margin-top: 20px;
  }
  
  .user-list {
    background-color: #f5f5f5;
    padding: 15px;
    border-radius: 8px;
  }
  
  .user-item {
    display: flex;
    align-items: center;
    justify-content: space-between;
    padding: 8px;
    margin-bottom: 8px;
    background-color: white;
    border-radius: 4px;
  }
  
  .voice-indicator {
    width: 8px;
    height: 8px;
    border-radius: 50%;
    background-color: #ccc;
  }
  
  .voice-indicator.active {
    background-color: #4CAF50;
    animation: pulse 1s infinite;
  }
  
  .voice-controls {
    grid-column: 1 / -1;
    display: flex;
    align-items: center;
    gap: 20px;
    padding: 15px;
    background-color: #f5f5f5;
    border-radius: 8px;
  }
  
  .mic-btn {
    padding: 12px 24px;
    border: none;
    border-radius: 25px;
    background-color: #2196F3;
    color: white;
    cursor: pointer;
    display: flex;
    align-items: center;
    gap: 8px;
    transition: all 0.3s ease;
  }
  
  .mic-btn.recording {
    background-color: #f44336;
    animation: pulse 1.5s infinite;
  }
  
  .audio-level-indicator {
    flex: 1;
    height: 8px;
    background-color: #ddd;
    border-radius: 4px;
    overflow: hidden;
  }
  
  .audio-level-bar {
    height: 100%;
    background: linear-gradient(90deg, #4CAF50 0%, #FFC107 50%, #F44336 100%);
    transition: width 0.1s ease;
  }
  
  .message-container {
    grid-column: 1 / -1;
    border: 1px solid #ddd;
    border-radius: 8px;
    overflow: hidden;
  }
  
  .message-list {
    height: 300px;
    overflow-y: auto;
    padding: 15px;
    background-color: #f9f9f9;
  }
  
  .message {
    margin-bottom: 10px;
    padding: 8px;
    background-color: white;
    border-radius: 4px;
    box-shadow: 0 1px 2px rgba(0,0,0,0.1);
  }
  
  .message-time {
    font-size: 0.8em;
    color: #666;
    margin-right: 8px;
  }
  
  @keyframes pulse {
    0% {
      transform: scale(1);
      opacity: 1;
    }
    50% {
      transform: scale(1.1);
      opacity: 0.8;
    }
    100% {
      transform: scale(1);
      opacity: 1;
    }
  }
  </style>